{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai-whisper\n",
      "  Using cached openai_whisper-20240930-py3-none-any.whl\n",
      "Collecting wtpsplit\n",
      "  Using cached wtpsplit-2.1.5-py3-none-any.whl.metadata (923 bytes)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting pydub\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting protobuf\n",
      "  Using cached protobuf-6.30.2-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Collecting blobfile\n",
      "  Using cached blobfile-3.0.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting ffmpeg\n",
      "  Using cached ffmpeg-1.4.tar.gz (5.1 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Using cached accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.2.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting ollama\n",
      "  Using cached ollama-0.4.7-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting python-telegram-bot\n",
      "  Using cached python_telegram_bot-22.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting numba (from openai-whisper)\n",
      "  Using cached numba-0.61.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.8 kB)\n",
      "Collecting numpy (from openai-whisper)\n",
      "  Using cached numpy-2.2.4-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting torch (from openai-whisper)\n",
      "  Using cached torch-2.6.0-cp311-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Collecting tqdm (from openai-whisper)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting more-itertools (from openai-whisper)\n",
      "  Using cached more_itertools-10.6.0-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting tiktoken (from openai-whisper)\n",
      "  Using cached tiktoken-0.9.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting huggingface-hub (from wtpsplit)\n",
      "  Using cached huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting scikit-learn>=1 (from wtpsplit)\n",
      "  Using cached scikit_learn-1.6.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Collecting skops (from wtpsplit)\n",
      "  Using cached skops-0.11.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting pandas>=1 (from wtpsplit)\n",
      "  Using cached pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Collecting cached_property (from wtpsplit)\n",
      "  Using cached cached_property-2.0.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting mosestokenizer (from wtpsplit)\n",
      "  Using cached mosestokenizer-1.2.1-py3-none-any.whl\n",
      "Collecting adapters>=1.0.1 (from wtpsplit)\n",
      "  Using cached adapters-1.1.1-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting pycryptodomex>=3.8 (from blobfile)\n",
      "  Using cached pycryptodomex-3.22.0-cp37-abi3-macosx_10_9_universal2.whl.metadata (3.4 kB)\n",
      "Collecting urllib3<3,>=1.25.3 (from blobfile)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting lxml>=4.9 (from blobfile)\n",
      "  Using cached lxml-5.3.2-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.11/site-packages (from accelerate) (7.0.0)\n",
      "Collecting httpx<0.29,>=0.27 (from ollama)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydantic<3.0.0,>=2.9.0 (from ollama)\n",
      "  Using cached pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting anyio (from httpx<0.29,>=0.27->ollama)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting certifi (from httpx<0.29,>=0.27->ollama)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<0.29,>=0.27->ollama)\n",
      "  Using cached httpcore-1.0.8-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx<0.29,>=0.27->ollama)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<0.29,>=0.27->ollama)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub->wtpsplit)\n",
      "  Using cached fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub->wtpsplit) (4.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas>=1->wtpsplit) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=1->wtpsplit)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1->wtpsplit)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.9.0->ollama)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.1 (from pydantic<3.0.0,>=2.9.0->ollama)\n",
      "  Using cached pydantic_core-2.33.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.9.0->ollama)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn>=1->wtpsplit)\n",
      "  Using cached scipy-1.15.2-cp311-cp311-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn>=1->wtpsplit)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1->wtpsplit)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting networkx (from torch->openai-whisper)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch->openai-whisper)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting sympy==1.13.1 (from torch->openai-whisper)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch->openai-whisper)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting docopt (from mosestokenizer->wtpsplit)\n",
      "  Using cached docopt-0.6.2-py2.py3-none-any.whl\n",
      "Collecting openfile (from mosestokenizer->wtpsplit)\n",
      "  Using cached openfile-0.0.7-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting uctools (from mosestokenizer->wtpsplit)\n",
      "  Using cached uctools-1.3.0-py3-none-any.whl\n",
      "Collecting toolwrapper (from mosestokenizer->wtpsplit)\n",
      "  Using cached toolwrapper-2.1.0-py3-none-any.whl\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba->openai-whisper)\n",
      "  Using cached llvmlite-0.44.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.8 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Using cached charset_normalizer-3.4.1-cp311-cp311-macosx_10_9_universal2.whl.metadata (35 kB)\n",
      "Collecting tabulate>=0.8.8 (from skops->wtpsplit)\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1->wtpsplit) (1.17.0)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<0.29,>=0.27->ollama)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch->openai-whisper)\n",
      "  Using cached MarkupSafe-3.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Using cached wtpsplit-2.1.5-py3-none-any.whl (124 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Using cached protobuf-6.30.2-cp39-abi3-macosx_10_9_universal2.whl (417 kB)\n",
      "Downloading blobfile-3.0.0-py3-none-any.whl (75 kB)\n",
      "Using cached accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Using cached sentencepiece-0.2.0-cp311-cp311-macosx_11_0_arm64.whl (1.2 MB)\n",
      "Downloading ollama-0.4.7-py3-none-any.whl (13 kB)\n",
      "Downloading python_telegram_bot-22.0-py3-none-any.whl (673 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m673.5/673.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached adapters-1.1.1-py3-none-any.whl (289 kB)\n",
      "Using cached transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.8-py3-none-any.whl (78 kB)\n",
      "Using cached huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading lxml-5.3.2-cp311-cp311-macosx_10_9_universal2.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached numpy-2.2.4-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
      "Using cached pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Downloading pycryptodomex-3.22.0-cp37-abi3-macosx_10_9_universal2.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.1-cp311-cp311-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl (172 kB)\n",
      "Using cached regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Using cached scikit_learn-1.6.1-cp311-cp311-macosx_12_0_arm64.whl (11.1 MB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Using cached torch-2.6.0-cp311-none-macosx_11_0_arm64.whl (66.5 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Using cached cached_property-2.0.1-py3-none-any.whl (7.4 kB)\n",
      "Using cached more_itertools-10.6.0-py3-none-any.whl (63 kB)\n",
      "Using cached numba-0.61.2-cp311-cp311-macosx_11_0_arm64.whl (2.8 MB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached skops-0.11.0-py3-none-any.whl (146 kB)\n",
      "Using cached tiktoken-0.9.0-cp311-cp311-macosx_11_0_arm64.whl (1.0 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp311-cp311-macosx_10_9_universal2.whl (194 kB)\n",
      "Using cached fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached llvmlite-0.44.0-cp311-cp311-macosx_11_0_arm64.whl (26.2 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached scipy-1.15.2-cp311-cp311-macosx_14_0_arm64.whl (22.4 MB)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached openfile-0.0.7-py3-none-any.whl (2.4 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp311-cp311-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Building wheels for collected packages: ffmpeg\n",
      "  Building wheel for ffmpeg (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6128 sha256=ef7c47846325f28c676c2f94d78a22325e0bf0aa65197a307eb77c0b0bd76c10\n",
      "  Stored in directory: /Users/zuzuka28/Library/Caches/pip/wheels/56/30/c5/576bdd729f3bc062d62a551be7fefd6ed2f761901568171e4e\n",
      "Successfully built ffmpeg\n",
      "Installing collected packages: toolwrapper, sentencepiece, pytz, pydub, openfile, mpmath, ffmpeg, docopt, urllib3, uctools, tzdata, typing-inspection, tqdm, threadpoolctl, tabulate, sympy, sniffio, safetensors, regex, pyyaml, python-dotenv, pydantic-core, pycryptodomex, protobuf, numpy, networkx, more-itertools, MarkupSafe, lxml, llvmlite, joblib, idna, h11, fsspec, filelock, charset-normalizer, certifi, cached_property, annotated-types, scipy, requests, pydantic, pandas, numba, mosestokenizer, jinja2, httpcore, blobfile, anyio, torch, tiktoken, scikit-learn, huggingface-hub, httpx, tokenizers, skops, python-telegram-bot, openai-whisper, ollama, accelerate, transformers, adapters, wtpsplit\n",
      "Successfully installed MarkupSafe-3.0.2 accelerate-1.6.0 adapters-1.1.1 annotated-types-0.7.0 anyio-4.9.0 blobfile-3.0.0 cached_property-2.0.1 certifi-2025.1.31 charset-normalizer-3.4.1 docopt-0.6.2 ffmpeg-1.4 filelock-3.18.0 fsspec-2025.3.2 h11-0.14.0 httpcore-1.0.8 httpx-0.28.1 huggingface-hub-0.30.2 idna-3.10 jinja2-3.1.6 joblib-1.4.2 llvmlite-0.44.0 lxml-5.3.2 more-itertools-10.6.0 mosestokenizer-1.2.1 mpmath-1.3.0 networkx-3.4.2 numba-0.61.2 numpy-2.2.4 ollama-0.4.7 openai-whisper-20240930 openfile-0.0.7 pandas-2.2.3 protobuf-6.30.2 pycryptodomex-3.22.0 pydantic-2.11.3 pydantic-core-2.33.1 pydub-0.25.1 python-dotenv-1.1.0 python-telegram-bot-22.0 pytz-2025.2 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.15.2 sentencepiece-0.2.0 skops-0.11.0 sniffio-1.3.1 sympy-1.13.1 tabulate-0.9.0 threadpoolctl-3.6.0 tiktoken-0.9.0 tokenizers-0.21.1 toolwrapper-2.1.0 torch-2.6.0 tqdm-4.67.1 transformers-4.48.3 typing-inspection-0.4.0 tzdata-2025.2 uctools-1.3.0 urllib3-2.4.0 wtpsplit-2.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install -U openai-whisper wtpsplit transformers pydub python-dotenv protobuf blobfile ffmpeg accelerate ffmpeg sentencepiece ollama python-telegram-bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 18:51:22,841 - audio_processor - INFO - Load Whisper (turbo)...\n",
      "2025-04-19 18:51:28,932 - audio_processor - INFO - Load SoT (sat-3l-sm)...\n",
      "2025-04-19 18:51:30,982 - audio_processor - INFO - Init TextPostProcessor with model: ilyagusev/saiga_llama3\n"
     ]
    }
   ],
   "source": [
    "from audio_processor import AppConfig, TranscriptionPipeline\n",
    "\n",
    "config = AppConfig(\n",
    "    whisper_model=\"turbo\",\n",
    "    llm_model=\"ilyagusev/saiga_llama3\"\n",
    ")\n",
    "\n",
    "pipeline = TranscriptionPipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 18:46:51,242 - audio_processor - INFO - Transcribe...\n",
      "/Users/zuzuka28/Documents/pp/audiodiary/.venv/lib/python3.11/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 8005/8005 [00:13<00:00, 580.32frames/s]\n",
      "2025-04-19 18:47:05,496 - audio_processor - INFO - Segmentation...\n",
      "2025-04-19 18:47:05,555 - audio_processor - INFO - Markdown formating...\n",
      "2025-04-19 18:47:12,985 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-04-19 18:47:13,005 - audio_processor - INFO - File saved: output/transcript_audio_1.wav_1745077633.md\n",
      "2025-04-19 18:47:13,006 - audio_processor - INFO - Processing Duration: 21.76s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'output/transcript_audio_1.wav_1745077633.md'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.run(\"audio_1.wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
